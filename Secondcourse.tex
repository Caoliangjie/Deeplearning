\documentclass[letterpaper]{article}
\usepackage{subfig}
\usepackage[english]{babel}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{amssymb}
\usepackage{outline} 
\usepackage{pmgraph} 
\usepackage[normalem]{ulem}
\usepackage{graphicx} 
\usepackage{verbatim}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\begin{document}
\title{
\begin{figure}[!htb]
 \centering
 \includegraphics[width=5in]{logo.png}\\
 \end{figure}First Course}
\author{Liangjie Cao   \\\\\\\\\\ VISION@OUC}
\date{\today}
\maketitle
\newpage
\section{Compution Graph}
Professor Wu told us to use graph to express the logistic process. One compution graph includes the forward process and the backward process. And he show a complete process to us in figure~\ref{Figure1}. Let us see some details. The input $a$, $b$ and $c$ are 5, 3 and 2 at first. When we plus 0.001 to $a$. Then $v$ will change to 11.001 and $J$ will change to 33.003. Just a little bit change can change the output. And from the backward process, it's a gradient descent process. As a Chinese student, the derivatives are familiar to us. 
\begin{figure}[!htb]
	\centering
	\includegraphics[width=5in]{Comp.png}\\
	\caption{The compution graph}
\end{figure}\label{Figure1}
\section{Vectorization}
When training m of the data, we can use for loop to make it. Professor Wu tells us to use the vectorization process save time. The typical vectorization process in python code is like sigmoid function $z=np.dot(w.T,x)+b$. Where np is an import of numpy, w and x are matrixs and np.dot is a process to compute two matrix. Then b is broadcasting to a vector. This process can save 30\% time to the for loop process. 
\section{Neural Network}
\begin{figure}[!htb]
	\centering
	\includegraphics[width=5in]{repre.png}\\
	\caption{Shallow neural network}
\end{figure}\label{Figure2}
Then come to the third part, professor Wu says we can form a neural network by stacking together a lot of little sigmoid units. As we can see from figure~\ref{Figure2}, it's a two layers of neural network. The input layer cannot be count into the layers. Then there're some new symbols to recognize. We can see the $z_{1}^{[1]}$ in figure~\ref{Figure2}, $[1]$ means this is in the second layer and $1$ means this is the first unit. This can make m samples compute at the same time. Activation fuctions is like the sigmoid fuctions and other typical functions. ReLu, tanh and leaky ReLu is some different activate function. All of these functions are non-linear fuctions. It means that if we use linear function, whatever layers do we have, we can only get the linear output at last.
\par The initial process may cause some probelms. If we make the weights too large, when we compute the functions like sigmoid function. W will cause too large z, what's more, the gradient descent process make a little sense in the process.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=5in]{denu.png}\\
	\caption{Deep neural network}
\end{figure}\label{Figure3}
\par Then it come the deep neural network(in figure~\ref{Figure3}), the main probelm is to make sure the demision of the compute matrix is correct. Professor Wu gives us his own method to check the demensions. When the compute unit is like sigmoid function, $z^(1)=W^(1)x+b^(1)$. W's row is equal to the backward units' row, and b's row is equal to W's. The column of b is $n^(0)$. And $n^(0)$ means the amount of the nodes in the input layer. 
\section{Hyperparameters}
The parameters which can control the size of the other parameters called the hyperparameters. Learning rates is a typical hyperparameter. This may depend on ones' own experience to choose a proper hyperparameters. Maybe we can chose 0.01 at first to change the learning rate step by step when we finally get an ideal output.
\section{Conclusion}
I know that this week my teammates get some works to do. My creed is learning better and doing better. I really know that the practice is important. To be honest, our foundation is really poor, we are trying to change the status these days. I try to help them deal with the work as well as I can do, and hope if one day I can do it best. 
\par This week, our senior teammates give us a report proceeding. I find maybe GAN is a really good direction for me. Actually I really like these creation works since my childhood. I will firstly realize the meaning of it and catch the main idea. I hope I can come up my own idea one day.
\end{document}